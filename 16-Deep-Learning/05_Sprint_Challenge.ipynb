{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Sprint Challenge\n",
    "### RNNs, CNNs, GANS, and AutoML\n",
    "\n",
    "In this Sprint Challenge, you'll explore some of the cutting edge of Data Science. *Caution* - these approaches can be pretty heavy computationally. All problems are designed to completed with 5-10 minutes of run time on most machines. If you approach takes longer, please double check your work. \n",
    "\n",
    "## Part 1 - RNNs\n",
    "\n",
    "Use an RNN to fit a classification model on tweets to distinguish from tweets from any two accounts. The following code sample illustrates how to access data from an account (no API auth needed), uses [twitterscraper](https://github.com/taspinar/twitterscraper): \n",
    "\n",
    "Your Tasks:\n",
    "* Select two twitter accounts to gather data from\n",
    "* Use twitterscraper to get ~1,000 tweets from each account\n",
    "* Encode the characters to a sequence of integers for the model\n",
    "* Get the data into the appropriate shape/format, including labels and a train/test split\n",
    "* Use Keras to fit a predictive model, classying tweets as being from one acount or the other\n",
    "* Report your overall score and accuracy\n",
    "\n",
    "For reference, the [Keras IMDB classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well as the RNN code we used in class.\n",
    "\n",
    "Note - focus on getting a running model, not on making accuracy with extreme data size or epoch numbers. Fit a baseline model based on tweet text. Only revisit and push accuracy or incorporate additional features if you get everything else done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:streetsblogchi since:2006-03-21 until:2006-11-16', 'from:streetsblogchi since:2006-11-16 until:2007-07-14', 'from:streetsblogchi since:2007-07-14 until:2008-03-10', 'from:streetsblogchi since:2008-03-10 until:2008-11-06', 'from:streetsblogchi since:2008-11-06 until:2009-07-04', 'from:streetsblogchi since:2009-07-04 until:2010-03-01', 'from:streetsblogchi since:2010-03-01 until:2010-10-27', 'from:streetsblogchi since:2010-10-27 until:2011-06-25', 'from:streetsblogchi since:2011-06-25 until:2012-02-20', 'from:streetsblogchi since:2012-02-20 until:2012-10-17', 'from:streetsblogchi since:2012-10-17 until:2013-06-14', 'from:streetsblogchi since:2013-06-14 until:2014-02-10', 'from:streetsblogchi since:2014-02-10 until:2014-10-08', 'from:streetsblogchi since:2014-10-08 until:2015-06-05', 'from:streetsblogchi since:2015-06-05 until:2016-01-31', 'from:streetsblogchi since:2016-01-31 until:2016-09-28', 'from:streetsblogchi since:2016-09-28 until:2017-05-26', 'from:streetsblogchi since:2017-05-26 until:2018-01-21', 'from:streetsblogchi since:2018-01-21 until:2018-09-18', 'from:streetsblogchi since:2018-09-18 until:2019-05-17']\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 26 tweets (26 new).\n",
      "INFO: Got 86 tweets (60 new).\n",
      "INFO: Got 146 tweets (60 new).\n",
      "INFO: Got 206 tweets (60 new).\n",
      "INFO: Got 266 tweets (60 new).\n",
      "INFO: Got 326 tweets (60 new).\n",
      "INFO: Got 386 tweets (60 new).\n",
      "INFO: Got 446 tweets (60 new).\n",
      "INFO: Got 506 tweets (60 new).\n",
      "INFO: Got 566 tweets (60 new).\n",
      "INFO: Got 626 tweets (60 new).\n",
      "INFO: Got 686 tweets (60 new).\n",
      "INFO: Got 746 tweets (60 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:elonmusk since:2006-03-21 until:2006-11-16', 'from:elonmusk since:2006-11-16 until:2007-07-14', 'from:elonmusk since:2007-07-14 until:2008-03-10', 'from:elonmusk since:2008-03-10 until:2008-11-06', 'from:elonmusk since:2008-11-06 until:2009-07-04', 'from:elonmusk since:2009-07-04 until:2010-03-01', 'from:elonmusk since:2010-03-01 until:2010-10-27', 'from:elonmusk since:2010-10-27 until:2011-06-25', 'from:elonmusk since:2011-06-25 until:2012-02-20', 'from:elonmusk since:2012-02-20 until:2012-10-17', 'from:elonmusk since:2012-10-17 until:2013-06-14', 'from:elonmusk since:2013-06-14 until:2014-02-10', 'from:elonmusk since:2014-02-10 until:2014-10-08', 'from:elonmusk since:2014-10-08 until:2015-06-05', 'from:elonmusk since:2015-06-05 until:2016-01-31', 'from:elonmusk since:2016-01-31 until:2016-09-28', 'from:elonmusk since:2016-09-28 until:2017-05-26', 'from:elonmusk since:2017-05-26 until:2018-01-21', 'from:elonmusk since:2018-01-21 until:2018-09-18', 'from:elonmusk since:2018-09-18 until:2019-05-17']\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 1 tweets (1 new).\n",
      "INFO: Got 97 tweets (96 new).\n",
      "INFO: Got 233 tweets (136 new).\n",
      "INFO: Got 375 tweets (142 new).\n",
      "INFO: Got 531 tweets (156 new).\n",
      "INFO: Got 731 tweets (200 new).\n",
      "INFO: Got 924 tweets (193 new).\n",
      "INFO: Got 1278 tweets (354 new).\n",
      "INFO: Got 1822 tweets (544 new).\n",
      "INFO: Got 2454 tweets (632 new).\n",
      "INFO: Got 3184 tweets (730 new).\n",
      "INFO: Got 4742 tweets (1558 new).\n",
      "INFO: Got 6592 tweets (1850 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6592\n"
     ]
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "\n",
    "tweets_1 = query_tweets('from:streetsblogchi', 1000)\n",
    "print(len(tweets_1))\n",
    "tweets_2 = query_tweets('from:elonmusk', 1000)\n",
    "print(len(tweets_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "import numpy as np\n",
    "\n",
    "tweet_chr_set = set()\n",
    "for user_tweets in [tweets_1, tweets_2]:\n",
    "    for tweet in user_tweets:\n",
    "        tweet_chr_set = tweet_chr_set.union(tweet.text)\n",
    "\n",
    "chr_to_int = dict((c, i) for i, c in enumerate(list(tweet_chr_set)))\n",
    "\n",
    "tweets = []\n",
    "for user_tweets in [tweets_1, tweets_2]:\n",
    "    for tweet in user_tweets:\n",
    "        tweets.append([chr_to_int[c] for c in tweet.text])\n",
    "        \n",
    "X = np.array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: streetsblogchi, 0: elonmusk\n",
    "y = np.array([1] * len(tweets_1) + [0] * len(tweets_2))\n",
    "y = y.reshape(len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([122, 76, 7, 50, 10, 41, 138, 10, 76, 10, 110, 56, 30, 149, 76, 130, 10, 74, 30, 10, 50, 130, 128, 30, 41, 132, 76, 149, 50, 10, 83, 99, 128, 120, 128, 56, 99, 130, 149, 13, 10, 95, 99, 130, 10, 74, 145, 99, 110, 10, 145, 76, 74, 129, 10, 145, 74, 74, 138, 37, 84, 84, 139, 129, 114, 138, 84, 139, 128, 149, 70, 50, 62, 10, 38, 2, 138, 50, 130, 10, 74, 30, 10, 72, 3, 10, 110, 74, 76, 74, 50, 110, 10, 27, 10, 81, 69, 134]),\n",
       "       list([51, 112, 10, 112, 50, 101, 50, 132, 76, 56, 10, 69, 122, 115, 77, 10, 114, 30, 130, 50, 120, 10, 101, 30, 50, 110, 130, 80, 74, 10, 138, 76, 120, 10, 74, 30, 10, 114, 76, 99, 130, 74, 76, 99, 130, 10, 83, 99, 7, 50, 10, 56, 76, 130, 50, 110, 13, 10, 95, 145, 30, 10, 101, 30, 50, 110, 75, 10, 145, 74, 74, 138, 37, 84, 84, 139, 129, 114, 138, 84, 114, 112, 26, 74, 85, 43, 10, 63, 83, 99, 128, 120, 128, 56, 99, 130, 149]),\n",
       "       list([115, 74, 10, 88, 41, 114, 76, 80, 110, 10, 112, 30, 132, 10, 74, 145, 50, 10, 112, 99, 132, 110, 74, 10, 74, 99, 114, 50, 129, 10, 38, 6, 10, 88, 41, 114, 76, 80, 110, 10, 69, 30, 132, 130, 50, 132, 10, 95, 84, 10, 6, 95, 50, 43, 56, 50, 50, 134, 10, 145, 74, 74, 138, 37, 84, 84, 85, 110, 119, 129, 128, 30, 114, 84, 99, 130, 103, 82, 142, 26])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X[0:3], '-' * 10, y[0:3], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4916, 100)\n",
      "x_test shape:  (2422, 100)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test  = sequence.pad_sequences(X_test,  maxlen=maxlen)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4916 samples, validate on 2422 samples\n",
      "Epoch 1/15\n",
      "4916/4916 [==============================] - 44s 9ms/step - loss: 0.2742 - acc: 0.9068 - val_loss: 0.2324 - val_acc: 0.9121\n",
      "Epoch 2/15\n",
      "4916/4916 [==============================] - 45s 9ms/step - loss: 0.1945 - acc: 0.9331 - val_loss: 0.2032 - val_acc: 0.9455\n",
      "Epoch 3/15\n",
      "4916/4916 [==============================] - 41s 8ms/step - loss: 0.1742 - acc: 0.9451 - val_loss: 0.1896 - val_acc: 0.9393\n",
      "Epoch 4/15\n",
      "4916/4916 [==============================] - 40s 8ms/step - loss: 0.1504 - acc: 0.9540 - val_loss: 0.2020 - val_acc: 0.9277\n",
      "Epoch 5/15\n",
      "4916/4916 [==============================] - 40s 8ms/step - loss: 0.1473 - acc: 0.9538 - val_loss: 0.1724 - val_acc: 0.9434\n",
      "Epoch 6/15\n",
      "4916/4916 [==============================] - 40s 8ms/step - loss: 0.1577 - acc: 0.9498 - val_loss: 0.1780 - val_acc: 0.9368\n",
      "Epoch 7/15\n",
      "4916/4916 [==============================] - 40s 8ms/step - loss: 0.1485 - acc: 0.9510 - val_loss: 0.1698 - val_acc: 0.9463\n",
      "Epoch 8/15\n",
      "4916/4916 [==============================] - 40s 8ms/step - loss: 0.1410 - acc: 0.9550 - val_loss: 0.1865 - val_acc: 0.9410\n",
      "Epoch 9/15\n",
      "4916/4916 [==============================] - 43s 9ms/step - loss: 0.1385 - acc: 0.9567 - val_loss: 0.1670 - val_acc: 0.9492\n",
      "Epoch 10/15\n",
      "4916/4916 [==============================] - 40s 8ms/step - loss: 0.1394 - acc: 0.9577 - val_loss: 0.1645 - val_acc: 0.9488\n",
      "Epoch 11/15\n",
      "4916/4916 [==============================] - 42s 9ms/step - loss: 0.1298 - acc: 0.9601 - val_loss: 0.1704 - val_acc: 0.9467\n",
      "Epoch 12/15\n",
      "4916/4916 [==============================] - 41s 8ms/step - loss: 0.1241 - acc: 0.9634 - val_loss: 0.1747 - val_acc: 0.9476\n",
      "Epoch 13/15\n",
      "4916/4916 [==============================] - 44s 9ms/step - loss: 0.1274 - acc: 0.9644 - val_loss: 0.1743 - val_acc: 0.9463\n",
      "Epoch 14/15\n",
      "4916/4916 [==============================] - 42s 8ms/step - loss: 0.1317 - acc: 0.9579 - val_loss: 0.1973 - val_acc: 0.9356\n",
      "Epoch 15/15\n",
      "4916/4916 [==============================] - 42s 8ms/step - loss: 0.1279 - acc: 0.9614 - val_loss: 0.1587 - val_acc: 0.9509\n",
      "2422/2422 [==============================] - 4s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_features = 20000  # or max(chr_to_int.values()) ?\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15867118511585832 0.9508670522692182\n"
     ]
    }
   ],
   "source": [
    "print(score, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - CNNs\n",
    "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs. You may need to adjust the number of images to query to ensure one picture contains a frog. Your goal is validly run ResNet50 on the input images - don't worry about tuning or improving the model. \n",
    "\n",
    "*Hint:* ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are bullfrog, tree frog, and tailed frog.\n",
    "\n",
    "Stretch goal - also check for fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item no.: 1 --> Item name = animal pond\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "Completed Image ====> 1.Pondanimals.GIF\n",
      "Completed Image ====> 2.hqdefault.jpg\n",
      "Completed Image ====> 3.water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
      "Completed Image ====> 4.PKLS4116_inline.png\n",
      "Completed Image ====> 5.alligator_animal_on_pond.jpg\n",
      "Completed Image ====> 6.frog-2243543_960_720.jpg\n",
      "Completed Image ====> 7.maxresdefault.jpg\n",
      "Completed Image ====> 8.birds-in-a-pond-5986310798966784.jpg\n",
      "\n",
      "Errors: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google_images_download import google_images_download\n",
    "\n",
    "response = google_images_download.googleimagesdownload()\n",
    "arguments = {'keywords': \"animal pond\",\n",
    "             \"limit\": 8, \n",
    "             \"print_urls\": False,\n",
    "             \"output_directory\":'img/ignore'}\n",
    "absolute_image_paths = response.download(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n03598930', 'jigsaw_puzzle', 0.86803204), ('n06359193', 'web_site', 0.06409999)]\n",
      "Predicted: [('n01443537', 'goldfish', 0.8495913), ('n01631663', 'eft', 0.067602284)]\n",
      "Predicted: [('n02442845', 'mink', 0.30976605), ('n02363005', 'beaver', 0.23398966)]\n",
      "Predicted: [('n04243546', 'slot', 0.8712447), ('n04476259', 'tray', 0.04993611)]\n",
      "Predicted: [('n01698640', 'American_alligator', 0.963947), ('n01697457', 'African_crocodile', 0.026759991)]\n",
      "Predicted: [('n01641577', 'bullfrog', 0.9223341), ('n01644900', 'tailed_frog', 0.07364755)]\n",
      "Predicted: [('n02013706', 'limpkin', 0.3572372), ('n01806567', 'quail', 0.1810789)]\n",
      "Predicted: [('n02009912', 'American_egret', 0.7822417), ('n02012849', 'crane', 0.1433928)]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "PTModel = ResNet50(weights='imagenet')\n",
    "\n",
    "for path in Path('img/ignore/animal pond').iterdir():\n",
    "    # match the shape of the image to the shape of the pretrained model\n",
    "    Img = image.load_img(path, target_size=(224, 224))\n",
    "    # preprocess the data\n",
    "    InputIMG = image.img_to_array(Img)\n",
    "    InputIMG = np.expand_dims(InputIMG, axis=0)\n",
    "    InputIMG = preprocess_input(InputIMG)\n",
    "    # use pre trained model to classify image\n",
    "    PredData = PTModel.predict(InputIMG)\n",
    "    # use decode_predictions to eval perfomance\n",
    "    print('Predicted:', decode_predictions(PredData, top=2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctly identified frog in image 6!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - AutoML\n",
    "\n",
    "Use [TPOT](https://epistasislab.github.io/tpot/) to fit a predictive model for the King County housing data, with `price` as the target output variable.\n",
    "\n",
    "As with previous questions, your goal is to run TPOT and successfully run and report error at the end. Also, in the interest of time, feel free to choose small `generation=1`and `population_size=10` parameters, so your pipeline runs efficiently. You will want to be able to iterate and test. \n",
    "\n",
    "*Hint:* You will have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running - as long as you still get a valid model with reasonable predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  condition  grade  sqft_above  \\\n",
       "0      5650     1.0           0     0          3      7        1180   \n",
       "1      7242     2.0           0     0          3      7        2170   \n",
       "2     10000     1.0           0     0          3      6         770   \n",
       "3      5000     1.0           0     0          5      7        1050   \n",
       "4      8080     1.0           0     0          3      8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/ryanleeallred/\"\n",
    "       \"datasets/master/kc_house_data.csv\")\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.580302e+09</td>\n",
       "      <td>5.400881e+05</td>\n",
       "      <td>3.370842</td>\n",
       "      <td>2.114757</td>\n",
       "      <td>2079.899736</td>\n",
       "      <td>1.510697e+04</td>\n",
       "      <td>1.494309</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.234303</td>\n",
       "      <td>3.409430</td>\n",
       "      <td>7.656873</td>\n",
       "      <td>1788.390691</td>\n",
       "      <td>291.509045</td>\n",
       "      <td>1971.005136</td>\n",
       "      <td>84.402258</td>\n",
       "      <td>98077.939805</td>\n",
       "      <td>47.560053</td>\n",
       "      <td>-122.213896</td>\n",
       "      <td>1986.552492</td>\n",
       "      <td>12768.455652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.876566e+09</td>\n",
       "      <td>3.671272e+05</td>\n",
       "      <td>0.930062</td>\n",
       "      <td>0.770163</td>\n",
       "      <td>918.440897</td>\n",
       "      <td>4.142051e+04</td>\n",
       "      <td>0.539989</td>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.766318</td>\n",
       "      <td>0.650743</td>\n",
       "      <td>1.175459</td>\n",
       "      <td>828.090978</td>\n",
       "      <td>442.575043</td>\n",
       "      <td>29.373411</td>\n",
       "      <td>401.679240</td>\n",
       "      <td>53.505026</td>\n",
       "      <td>0.138564</td>\n",
       "      <td>0.140828</td>\n",
       "      <td>685.391304</td>\n",
       "      <td>27304.179631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000102e+06</td>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.519000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.123049e+09</td>\n",
       "      <td>3.219500e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1427.000000</td>\n",
       "      <td>5.040000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1951.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98033.000000</td>\n",
       "      <td>47.471000</td>\n",
       "      <td>-122.328000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>5100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.904930e+09</td>\n",
       "      <td>4.500000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>7.618000e+03</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98065.000000</td>\n",
       "      <td>47.571800</td>\n",
       "      <td>-122.230000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>7620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.308900e+09</td>\n",
       "      <td>6.450000e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.068800e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98118.000000</td>\n",
       "      <td>47.678000</td>\n",
       "      <td>-122.125000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.900000e+09</td>\n",
       "      <td>7.700000e+06</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13540.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9410.000000</td>\n",
       "      <td>4820.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>871200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price      bedrooms     bathrooms   sqft_living  \\\n",
       "count  2.161300e+04  2.161300e+04  21613.000000  21613.000000  21613.000000   \n",
       "mean   4.580302e+09  5.400881e+05      3.370842      2.114757   2079.899736   \n",
       "std    2.876566e+09  3.671272e+05      0.930062      0.770163    918.440897   \n",
       "min    1.000102e+06  7.500000e+04      0.000000      0.000000    290.000000   \n",
       "25%    2.123049e+09  3.219500e+05      3.000000      1.750000   1427.000000   \n",
       "50%    3.904930e+09  4.500000e+05      3.000000      2.250000   1910.000000   \n",
       "75%    7.308900e+09  6.450000e+05      4.000000      2.500000   2550.000000   \n",
       "max    9.900000e+09  7.700000e+06     33.000000      8.000000  13540.000000   \n",
       "\n",
       "           sqft_lot        floors    waterfront          view     condition  \\\n",
       "count  2.161300e+04  21613.000000  21613.000000  21613.000000  21613.000000   \n",
       "mean   1.510697e+04      1.494309      0.007542      0.234303      3.409430   \n",
       "std    4.142051e+04      0.539989      0.086517      0.766318      0.650743   \n",
       "min    5.200000e+02      1.000000      0.000000      0.000000      1.000000   \n",
       "25%    5.040000e+03      1.000000      0.000000      0.000000      3.000000   \n",
       "50%    7.618000e+03      1.500000      0.000000      0.000000      3.000000   \n",
       "75%    1.068800e+04      2.000000      0.000000      0.000000      4.000000   \n",
       "max    1.651359e+06      3.500000      1.000000      4.000000      5.000000   \n",
       "\n",
       "              grade    sqft_above  sqft_basement      yr_built  yr_renovated  \\\n",
       "count  21613.000000  21613.000000   21613.000000  21613.000000  21613.000000   \n",
       "mean       7.656873   1788.390691     291.509045   1971.005136     84.402258   \n",
       "std        1.175459    828.090978     442.575043     29.373411    401.679240   \n",
       "min        1.000000    290.000000       0.000000   1900.000000      0.000000   \n",
       "25%        7.000000   1190.000000       0.000000   1951.000000      0.000000   \n",
       "50%        7.000000   1560.000000       0.000000   1975.000000      0.000000   \n",
       "75%        8.000000   2210.000000     560.000000   1997.000000      0.000000   \n",
       "max       13.000000   9410.000000    4820.000000   2015.000000   2015.000000   \n",
       "\n",
       "            zipcode           lat          long  sqft_living15     sqft_lot15  \n",
       "count  21613.000000  21613.000000  21613.000000   21613.000000   21613.000000  \n",
       "mean   98077.939805     47.560053   -122.213896    1986.552492   12768.455652  \n",
       "std       53.505026      0.138564      0.140828     685.391304   27304.179631  \n",
       "min    98001.000000     47.155900   -122.519000     399.000000     651.000000  \n",
       "25%    98033.000000     47.471000   -122.328000    1490.000000    5100.000000  \n",
       "50%    98065.000000     47.571800   -122.230000    1840.000000    7620.000000  \n",
       "75%    98118.000000     47.678000   -122.125000    2360.000000   10083.000000  \n",
       "max    98199.000000     47.777600   -121.315000    6210.000000  871200.000000  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 int64\n",
       "date              object\n",
       "price            float64\n",
       "bedrooms           int64\n",
       "bathrooms        float64\n",
       "sqft_living        int64\n",
       "sqft_lot           int64\n",
       "floors           float64\n",
       "waterfront         int64\n",
       "view               int64\n",
       "condition          int64\n",
       "grade              int64\n",
       "sqft_above         int64\n",
       "sqft_basement      int64\n",
       "yr_built           int64\n",
       "yr_renovated       int64\n",
       "zipcode            int64\n",
       "lat              float64\n",
       "long             float64\n",
       "sqft_living15      int64\n",
       "sqft_lot15         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "date                 0\n",
       "price                0\n",
       "bedrooms            13\n",
       "bathrooms           10\n",
       "sqft_living          0\n",
       "sqft_lot             0\n",
       "floors               0\n",
       "waterfront       21450\n",
       "view             19489\n",
       "condition            0\n",
       "grade                0\n",
       "sqft_above           0\n",
       "sqft_basement    13126\n",
       "yr_built             0\n",
       "yr_renovated     20699\n",
       "zipcode              0\n",
       "lat                  0\n",
       "long                 0\n",
       "sqft_living15        0\n",
       "sqft_lot15           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id', 'date', 'zipcode', 'yr_renovated', 'lat', 'long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'price'\n",
    "X = df.drop(columns=target).values\n",
    "y = df[target].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "pipeline_optimizer = TPOTRegressor(\n",
    "    generations=1, population_size=20, cv=3, n_jobs=-1,\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pipeline: ExtraTreesRegressor(input_matrix, bootstrap=False, max_features=0.55, min_samples_leaf=1, min_samples_split=6, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTRegressor(config_dict=None, crossover_rate=0.1, cv=3,\n",
       "       disable_update_check=False, early_stop=None, generations=1,\n",
       "       max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "       mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n",
       "       periodic_checkpoint_folder=None, population_size=20,\n",
       "       random_state=None, scoring=None, subsample=1.0,\n",
       "       template='RandomTree', use_dask=False, verbosity=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_optimizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: -40550329466.80189\n",
      "rmse: 201371.1237163906\n"
     ]
    }
   ],
   "source": [
    "print('mse: ', pipeline_optimizer.score(X_test, y_test))\n",
    "print('rmse:', np.sqrt(pipeline_optimizer.score(X_test, y_test) * -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - More... \n",
    "\n",
    "Answer the following questions, with a target audience of a fellow Data Scientist. A few sentences per answer is fine. Only elaborate if time allows. Use markdown to format your answers.\n",
    "\n",
    "**What do you consider your strongest area as a Data Scientist?**\n",
    "  * Since ETL demands the greatest human effort, I image that will always be my greatest strength. However I also have a great eye for design and presentation. I scaffold my projects well to efficiently apply effort to a given task.\n",
    "\n",
    "**What area of Data Science would you most like to learn more about and why?**\n",
    "  * Computer vision and geospatial algorithms. I love solutions that are tangible, and these applications seem to exist in the real world. They augment aspects of humanity (vision and physical engagement with our world) in ways that I find satisfying and mystifying at the same time.\n",
    "\n",
    "**Where do you think Data Science will be in 5 years?**\n",
    "  * I think ETL tasks will still require the most work. Automated modeling will be easier. Pre-trained models will be more powerful and generalizable, and new frameworks will arise. Python will explode in usership. The barrier to entry will continue to decline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
