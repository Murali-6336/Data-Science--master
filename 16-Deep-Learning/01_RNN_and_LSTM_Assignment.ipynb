{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and LSTM Assignment\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg\" width=400px>\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!\n",
    "\n",
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNN_NLP_Generator(object):\n",
    "    \"\"\"RNN with one hidden layer\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward_prop(self, inputs, targets, h_prev):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(len(inputs)): # t is a \"time step\" and is used as a dict key\n",
    "            xs[t] = np.zeros((self.num_chars,1))\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(np.dot(self.W_xh, xs[t]) + np.dot(self.W_hh, hs[t-1]) + self.b_h)\n",
    "            ys[t] = np.dot(self.W_hy, hs[t]) + self.b_y # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "\n",
    "            # Softmax\n",
    "            loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "        return loss, ps, hs, xs\n",
    "\n",
    "    def backward_prop(self, ps, inputs, hs, xs, targets):\n",
    "        # make all zero matrices\n",
    "        dWxh, dWhh, dWhy, dbh, dby, dhnext = \\\n",
    "            [np.zeros_like(_) for _ in [self.W_xh, self.W_hh, self.W_hy, \n",
    "                                        self.b_h,  self.b_y,  hs[0]]]\n",
    "        # reversed\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "            dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy \n",
    "            dh = np.dot(self.W_hy.T, dy) + dhnext # backprop into h. \n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            dhnext = np.dot(self.W_hh.T, dhraw)\n",
    "        \n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "\n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "    def train(self, article_text, hidden_size=500, n_iterations=1000, sequence_length=40, learning_rate=1e-1):\n",
    "        self.article_text = article_text\n",
    "        self.hidden_size = hidden_size\n",
    "        chars = list(set(article_text))\n",
    "        self.num_chars = len(chars)\n",
    "        self.char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "        self.int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "        # original text encoded as integers, what we pass into our model\n",
    "        self.integer_encoded = [self.char_to_int[i] for i in article_text]\n",
    "\n",
    "        # Weights\n",
    "        self.W_xh = np.random.randn(hidden_size, self.num_chars) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(self.num_chars, hidden_size) * 0.01\n",
    "        # biases\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        self.b_y = np.zeros((self.num_chars, 1))\n",
    "        # previous state\n",
    "        self.h_prev  = np.zeros((hidden_size, 1)) # h_(t-1)\n",
    "        \n",
    "        batch_size = round((len(article_text) / sequence_length) + 0.5) # math.ceil\n",
    "        data_pointer = 0\n",
    "\n",
    "        # memory variables for Adagrad\n",
    "        mWxh, mWhh, mWhy, mbh, mby = \\\n",
    "            [np.zeros_like(_) for _ in [self.W_xh, self.W_hh, self.W_hy, \n",
    "                                        self.b_h,  self.b_y]]\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            h_prev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "            data_pointer = 0 # go from start of data\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                inputs = [self.char_to_int[ch] \n",
    "                          for ch in self.article_text[data_pointer:data_pointer+sequence_length]]\n",
    "                targets = [self.char_to_int[ch] \n",
    "                           for ch in self.article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "\n",
    "                if (data_pointer+sequence_length+1 >= len(self.article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
    "                    targets.append(self.char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "                loss, ps, hs, xs = self.forward_prop(inputs, targets, h_prev)\n",
    "                dWxh, dWhh, dWhy, dbh, dby = self.backward_prop(ps, inputs, hs, xs, targets) \n",
    "\n",
    "                # perform parameter update with Adagrad\n",
    "                for param, dparam, mem in zip([self.W_xh, self.W_hh, self.W_hy, \n",
    "                                               self.b_h,  self.b_y], \n",
    "                                              [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                              [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "                    mem += dparam * dparam # elementwise\n",
    "                    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "\n",
    "                data_pointer += sequence_length # move data pointer\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print ('iter %d, loss: %f' % (i, loss)) # print progress\n",
    "                \n",
    "    def predict(self, test_char, length):\n",
    "        x = np.zeros((self.num_chars, 1))\n",
    "        x[self.char_to_int[test_char]] = 1\n",
    "        ixes = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for t in range(length):\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            y = np.dot(self.W_hy, h) + self.b_y\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.num_chars), p=p.ravel()) # ravel -> rank0\n",
    "            # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "            x = np.zeros((self.num_chars, 1)) # init\n",
    "            x[ix] = 1 \n",
    "            ixes.append(ix) # list\n",
    "        \n",
    "        txt = test_char + ''.join(self.int_to_char[i] for i in ixes)\n",
    "        \n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "r = requests.get(url)\n",
    "\n",
    "# subsample\n",
    "start_i = 2965  # index of first text, THE SONNETS\n",
    "length = 4000\n",
    "article_text = r.text[start_i:start_i+length]\n",
    "article_text = ' '.join(article_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_NLP_Generator()\n",
    "model.train(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_char='T', length=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
