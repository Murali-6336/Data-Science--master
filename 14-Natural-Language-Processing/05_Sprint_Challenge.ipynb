{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Natural Language Processing Sprint Challenge\n",
    "\n",
    "**Part 1 - Working with Text Data**\n",
    "Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string that has a lot of extra whitespace.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(whitespace_string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March 8, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March 15, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March 29, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April 5, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April 12, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April 19, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April 26, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May 10, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May 17, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May 24, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May 31, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June 7, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June 21, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June 28, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July 5, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July 12, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July 19, 2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dates\n",
       "0    March 8, 2015\n",
       "1   March 15, 2015\n",
       "2   March 22, 2015\n",
       "3   March 29, 2015\n",
       "4    April 5, 2015\n",
       "5   April 12, 2015\n",
       "6   April 19, 2015\n",
       "7   April 26, 2015\n",
       "8      May 3, 2015\n",
       "9     May 10, 2015\n",
       "10    May 17, 2015\n",
       "11    May 24, 2015\n",
       "12    May 31, 2015\n",
       "13    June 7, 2015\n",
       "14   June 14, 2015\n",
       "15   June 21, 2015\n",
       "16   June 28, 2015\n",
       "17    July 5, 2015\n",
       "18   July 12, 2015\n",
       "19   July 19, 2015"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "r = requests.get(\"https://raw.githubusercontent.com/\"\n",
    "                 \"ryanleeallred/datasets/master/dates.txt\")\n",
    "dates = re.findall(r'[a-zA-Z]+ \\d+, \\d{4}', r.text)\n",
    "pd.DataFrame({'dates':dates})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\City_Year\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize_words(doc):\n",
    "    import re\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    \n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize by word\n",
    "    tokens = word_tokenize(doc)\n",
    "    # Strip punctuation from within words\n",
    "    tokens = [x.lower().translate(table) \n",
    "              for x in tokens]\n",
    "    # Remove words with numbers\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Remove stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [sad, apl, friend]\n",
       "1                         [missed, new, moon, trailer]\n",
       "2                                       [omg, already]\n",
       "3    [omgaga, im, sooo, im, gunna, cry, dentist, si...\n",
       "4                        [think, mi, bf, cheating, tt]\n",
       "Name: SentimentText_token, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/\"\n",
    "                 \"datasets/master/twitter_sentiment_binary.csv\")\n",
    "\n",
    "df['SentimentText_token'] = df['SentimentText'].apply(tokenize_words)\n",
    "\n",
    "df['SentimentText_token'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "tf-idf scores represent how unique a given term is to a given document. The greater the score, the more characteristic that term is of the document in which it's present.\n",
    "\n",
    "tf-idf is calculated by the count of a term in a document, divided by the total words in that document, multipied by a log of the total number of documents divided by the number of documents containing the term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    " - Stretch goal: Track your results in a DataFrmae and produce a visualization of the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB # Multinomial Naive Bayes\n",
    "\n",
    "\n",
    "def vectorize(vectorizer, X_train, X_test):\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train_vect = vectorizer.transform(X_train)\n",
    "    X_train_trans = pd.DataFrame(X_train_vect.toarray(), \n",
    "                                 columns=vectorizer.get_feature_names())\n",
    "    X_test_trans = vectorizer.transform(X_test)\n",
    "\n",
    "    X_test_trans = pd.DataFrame(X_test_trans.toarray(), \n",
    "                                columns=vectorizer.get_feature_names())\n",
    "\n",
    "    return X_train_trans, X_test_trans, vectorizer\n",
    "\n",
    "\n",
    "def assess_model(model, X_train, X_test, \n",
    "                 y_train, y_test):\n",
    "    \"\"\"Expects transformed inputs\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    result = {}\n",
    "    result['model'] = str(model).split('(')[0]\n",
    "    result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "    result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "    print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['SentimentText_token'].apply(lambda x: \" \".join(x))\n",
    "y = df['Sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 20)\n",
    "X_train, X_test, _ = vectorize(tfidf, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'LogisticRegression', 'acc_train': 0.5960795589503819, 'acc_test': 0.6033103310331033}\n",
      "{'model': 'MultinomialNB', 'acc_train': 0.5958295308222175, 'acc_test': 0.6047604760476047}\n",
      "{'model': 'RandomForestClassifier', 'acc_train': 0.6105186833518771, 'acc_test': 0.6002600260026003}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.60331</td>\n",
       "      <td>0.596080</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.60476</td>\n",
       "      <td>0.595830</td>\n",
       "      <td>MultinomialNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.60026</td>\n",
       "      <td>0.610519</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_test  acc_train                   model\n",
       "0   0.60331   0.596080      LogisticRegression\n",
       "1   0.60476   0.595830           MultinomialNB\n",
       "2   0.60026   0.610519  RandomForestClassifier"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [LogisticRegression(solver='lbfgs'),\n",
    "               MultinomialNB(),\n",
    "               RandomForestClassifier(n_estimators=100)]\n",
    "\n",
    "results = []\n",
    "for model in classifiers:\n",
    "    result = assess_model(\n",
    "        model,\n",
    "        X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    df['SentimentText_token'],\n",
    "    min_count=1, size=5)\n",
    "\n",
    "model.wv.most_similar('twitter')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
